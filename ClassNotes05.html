<!doctype html>
<html lang="zh-CN">
 <head> 
  <meta charset="utf-8"> 
  <link rel="canonical" href="https://blog.csdn.net/qq_64671439/article/details/135345465"> 
  <meta http-equiv="content-type" content="text/html; charset=utf-8"> 
  <meta name="renderer" content="webkit"> 
  <meta name="force-rendering" content="webkit"> 
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> 
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"> 
  <meta name="report" content="{&quot;pid&quot;: &quot;blog&quot;, &quot;spm&quot;:&quot;1001.2101&quot;}"> 
  <meta name="referrer" content="always"> 
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <link rel="alternate" media="handheld" href="#">  
  <meta name="applicable-device" content="pc"> 
  <link href="https://g.csdnimg.cn/static/logo/favicon32.ico" rel="shortcut icon" type="image/x-icon"> 
  <title>【强化学习的数学原理-赵世钰】课程笔记（五）蒙特卡洛方法_强化学习的数学原理赵世钰-CSDN博客</title>   
  <meta name="keywords" content="强化学习的数学原理赵世钰"> 
  <meta name="csdn-baidu-search" content="{&quot;autorun&quot;:true,&quot;install&quot;:true,&quot;keyword&quot;:&quot;强化学习的数学原理赵世钰&quot;}"> 
  <meta name="description" content="文章浏览阅读4.3k次，点赞52次，收藏55次。万字长文，详细介绍强化学习中的蒙特卡洛方法，会持续更新_强化学习的数学原理赵世钰"> 
  <link rel="stylesheet" type="text/css" href="https://csdnimg.cn/release/blogv2/dist/pc/css/detail_enter-d4fc849858.min.css">  
  <link rel="stylesheet" type="text/css" href="https://csdnimg.cn/release/blogv2/dist/pc/themesSkin/skin-1024/skin-1024-ecd36efea2.min.css">    
  <meta name="toolbar" content="{&quot;type&quot;:&quot;0&quot;,&quot;fixModel&quot;:&quot;1&quot;}">    
  <link rel="stylesheet" type="text/css" href="https://csdnimg.cn/public/sandalstrap/1.4/css/sandalstrap.min.css"> 
  <style>
        .MathJax, .MathJax_Message, .MathJax_Preview{
            display: none
        }
    </style>    
 	<style>
	main div.blog-content-box pre {
		max-height: 100%;
		overflow-y: hidden;
	}
	</style>
 </head>  
 <body class="nodata  " style=""> 
  <div id="toolbarBox" style="min-height: 48px;"></div>    
  <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/pc/css/blog_code-01256533b5.min.css"> 
  <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/chart-3456820cac.css"> 
  <link rel="stylesheet" href="https://g.csdnimg.cn/lib/swiper/6.0.4/css/swiper.css">   
  <div class="main_father clearfix d-flex justify-content-center mainfather-concision" style="height:100%;"> 
   <div class="container clearfix container-concision" id="mainBox">  
    <main>  
     <div class="blog-content-box"> 
      <div class="article-header-box"> 
       <div class="article-header"> 
        <div class="article-title-box"> 
         <h1 class="title-article" id="articleContentId">【强化学习的数学原理-赵世钰】课程笔记（五）蒙特卡洛方法</h1> 
        </div> 
        <div class="article-info-box"> 
         <div class="article-bar-top"> 
          <img class="article-type-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/original.png" alt=""> 
          <div class="bar-content"> 
           <a class="follow-nickName " href="https://blog.csdn.net/qq_64671439" target="_blank" rel="noopener" title="leaf_leaves_leaf">leaf_leaves_leaf</a> 
           <img class="article-time-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newUpTime2.png" alt=""> 
           <span class="time">已于&nbsp;2024-03-03 13:40:18&nbsp;修改</span> 
           <div class="read-count-box"> 
            <img class="article-read-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes2.png" alt=""> 
            <span class="read-count">阅读量4.3k</span> 
            <a id="blog_detail_zk_collection" class="un-collection" data-report-click="{&quot;mod&quot;:&quot;popu_823&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4232&quot;,&quot;ab&quot;:&quot;new&quot;}"> <img class="article-collect-img article-heard-img un-collect-status isdefault" style="display:inline-block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect2.png" alt=""> <img class="article-collect-img article-heard-img collect-status isactive" style="display:none" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollectionActive2.png" alt=""> <span class="name">收藏</span> <span class="get-collection"> 55 </span> </a> 
            <div class="read-count-box is-like" data-type="top"> 
             <img class="article-read-img article-heard-img" style="display:none" id="is-like-imgactive-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Active.png" alt=""> 
             <img class="article-read-img article-heard-img" style="display:block" id="is-like-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Black.png" alt=""> 
             <span class="read-count" id="blog-digg-num">点赞数 52 </span> 
            </div> 
           </div> 
          </div> 
         </div> 
         <div class="blog-tags-box"> 
          <div class="tags-box artic-tag-box"> 
           <span class="label">分类专栏：</span> 
           <a class="tag-link" href="https://blog.csdn.net/qq_64671439/category_12540921.html" target="_blank" rel="noopener">【强化学习的数学原理-赵世钰】课程笔记</a> 
           <span class="label">文章标签：</span> 
           <a rel="noopener" data-report-query="spm=1001.2101.3001.4223" data-report-click="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;笔记&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;笔记\&quot;}&quot;}" data-report-view="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;笔记&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;笔记\&quot;}&quot;}" class="tag-link" href="https://so.csdn.net/so/search/s.do?q=%E7%AC%94%E8%AE%B0&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank">笔记</a> 
           <a rel="noopener" data-report-query="spm=1001.2101.3001.4223" data-report-click="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;机器学习&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;机器学习\&quot;}&quot;}" data-report-view="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;机器学习&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;机器学习\&quot;}&quot;}" class="tag-link" href="https://so.csdn.net/so/search/s.do?q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank">机器学习</a> 
           <a rel="noopener" data-report-query="spm=1001.2101.3001.4223" data-report-click="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;人工智能&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;人工智能\&quot;}&quot;}" data-report-view="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;人工智能&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;人工智能\&quot;}&quot;}" class="tag-link" href="https://so.csdn.net/so/search/s.do?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank">人工智能</a> 
           <a rel="noopener" data-report-query="spm=1001.2101.3001.4223" data-report-click="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;学习&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;学习\&quot;}&quot;}" data-report-view="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;学习&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;学习\&quot;}&quot;}" class="tag-link" href="https://so.csdn.net/so/search/s.do?q=%E5%AD%A6%E4%B9%A0&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank">学习</a> 
          </div> 
         </div> 
         <div class="up-time">
          <span>于&nbsp;2024-01-03 21:27:01&nbsp;首次发布</span>
         </div> 
         <div class="slide-content-box"> 
          <div class="article-copyright"> 
           <div class="creativecommons">
             版权声明：本文为博主原创文章，遵循
            <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。 
           </div> 
           <div class="article-source-link">
             本文链接：
            <a href="https://blog.csdn.net/qq_64671439/article/details/135345465" target="_blank">https://blog.csdn.net/qq_64671439/article/details/135345465</a> 
           </div> 
          </div> 
         </div> 
         <div class="operating"> 
          <a class="href-article-edit slide-toggle">版权</a> 
         </div> 
        </div> 
       </div> 
      </div> 
      <div id="blogHuaweiyunAdvert"></div> 
      <div id="blogColumnPayAdvert"> 
       <div class="column-group"> 
        <div class="column-group-item column-group0 column-group-item-one"> 
         <div class="item-l"> 
          <a class="item-target" href="https://blog.csdn.net/qq_64671439/category_12540921.html" target="_blank" title="【强化学习的数学原理-赵世钰】课程笔记" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.6332&quot;}" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.6332&quot;}"> <img class="item-target" src="https://i-blog.csdnimg.cn/columns/default/20201014180756916.png?x-oss-process=image/resize,m_fixed,h_224,w_224" alt=""> <span class="title item-target"> <span> <span class="tit">【强化学习的数学原理-赵世钰】课程笔记</span> <span class="dec">专栏收录该内容</span> </span> </span> </a> 
         </div> 
         <div class="item-m"> 
          <span>10 篇文章</span> 
         </div> 
         <div class="item-r"> 
          <a class="item-target article-column-bt articleColumnFreeBt" data-id="12540921">订阅专栏</a> 
         </div> 
        </div> 
       </div> 
      </div> 
      <article class="baidu_pl"> 
       <div id="article_content" class="article_content clearfix"> 
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/kdoc_html_views-1a98987dfd.css"> 
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/ck_htmledit_views-704d5b9767.css"> 
        <div id="content_views" class="htmledit_views atom-one-dark"> 
         <p id="main-toc"><strong>目录</strong></p> 
         <p id="-toc" style="margin-left:0px;"></p> 
         <p id="%C2%A0%E4%B8%80.%E5%86%85%E5%AE%B9%E6%A6%82%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%C2%A0%E4%B8%80.%E5%86%85%E5%AE%B9%E6%A6%82%E8%BF%B0" rel="nofollow">&nbsp;一.内容概述</a></p> 
         <p id="%E4%BA%8C.%E6%BF%80%E5%8A%B1%E6%80%A7%E5%AE%9E%E4%BE%8B%EF%BC%88Motivating%20examples%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%BA%8C.%E6%BF%80%E5%8A%B1%E6%80%A7%E5%AE%9E%E4%BE%8B%EF%BC%88Motivating%20examples%EF%BC%89" rel="nofollow">二.激励性实例（Motivating examples）</a></p> 
         <p id="%E4%B8%89.%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%9F%BA%E4%BA%8E%20MC%20%E7%9A%84%20RL%20%E7%AE%97%E6%B3%95%EF%BC%9AMC%20basic-toc" style="margin-left:0px;"><a href="#%E4%B8%89.%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%9F%BA%E4%BA%8E%20MC%20%E7%9A%84%20RL%20%E7%AE%97%E6%B3%95%EF%BC%9AMC%20basic" rel="nofollow">三.最简单的基于 MC 的 RL 算法：MC basic</a></p> 
         <p id="1.%E5%B0%86%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%97%A0%E6%A8%A1%E5%9E%8B%E8%BF%AD%E4%BB%A3%EF%BC%88Convert%20policy%20iteration%20to%20be%20model-free%EF%BC%89-toc" style="margin-left:40px;"><a href="#1.%E5%B0%86%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%97%A0%E6%A8%A1%E5%9E%8B%E8%BF%AD%E4%BB%A3%EF%BC%88Convert%20policy%20iteration%20to%20be%20model-free%EF%BC%89" rel="nofollow">1.将策略迭代转换为无模型迭代（Convert policy iteration to be model-free）</a></p> 
         <p id="2.The%20MC%20Basic%20algorithm-toc" style="margin-left:40px;"><a href="#2.The%20MC%20Basic%20algorithm" rel="nofollow">2.The MC Basic algorithm</a></p> 
         <p id="3.%E4%BE%8B%E5%AD%90-toc" style="margin-left:40px;"><a href="#3.%E4%BE%8B%E5%AD%90" rel="nofollow">3.例子</a></p> 
         <p id="%EF%BC%881%EF%BC%89%E4%BE%8B%E5%AD%90%201-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E4%BE%8B%E5%AD%90%201" rel="nofollow">（1）例子 1</a></p> 
         <p id="%EF%BC%882%EF%BC%89%E4%BE%8B%E5%AD%902-toc" style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89%E4%BE%8B%E5%AD%902" rel="nofollow">（2）例子2</a></p> 
         <p id="%E5%9B%9B.%E6%9B%B4%E9%AB%98%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%EF%BC%88Use%20data%20more%20efficiently%EF%BC%89%EF%BC%9AMC%20Exploring%20Starts-toc" style="margin-left:0px;"><a href="#%E5%9B%9B.%E6%9B%B4%E9%AB%98%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%EF%BC%88Use%20data%20more%20efficiently%EF%BC%89%EF%BC%9AMC%20Exploring%20Starts" rel="nofollow">四.更高效地使用数据（Use data more efficiently）：MC Exploring Starts</a></p> 
         <p id="%E4%BA%94.MC%20%E6%B2%A1%E6%9C%89%E6%8E%A2%E7%B4%A2%E5%B0%B1%E5%90%AF%E5%8A%A8%EF%BC%88MC%20without%20exploring%20starts%EF%BC%89%EF%BC%9AAlgorithm%3A%20MC%20%CE%B5-Greedy-toc" style="margin-left:0px;"><a href="#%E4%BA%94.MC%20%E6%B2%A1%E6%9C%89%E6%8E%A2%E7%B4%A2%E5%B0%B1%E5%90%AF%E5%8A%A8%EF%BC%88MC%20without%20exploring%20starts%EF%BC%89%EF%BC%9AAlgorithm%3A%20MC%20%CE%B5-Greedy" rel="nofollow">五.MC 没有探索就启动（MC without exploring starts）：Algorithm: MC ε-Greedy</a></p> 
         <p id="1.%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D-toc" style="margin-left:40px;"><a href="#1.%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D" rel="nofollow">1.算法介绍</a></p> 
         <p id="2.%E4%BE%8B%E5%AD%90-toc" style="margin-left:40px;"><a href="#2.%E4%BE%8B%E5%AD%90" rel="nofollow">2.例子</a></p> 
         <p id="%C2%A0%E5%85%AD.%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%C2%A0%E5%85%AD.%E6%80%BB%E7%BB%93" rel="nofollow">&nbsp;六.总结</a></p> 
         <hr id="hr-toc"> 
         <p></p> 
         <h3 id="%C2%A0%E4%B8%80.%E5%86%85%E5%AE%B9%E6%A6%82%E8%BF%B0">&nbsp;一.内容概述</h3> 
         <ul>
          <li>上节课介绍了 model-base 的方法，这节课将介绍 model-free 的方法，上节课的 policy iteration 的方法是这节课的基础，我们把 policy iteration 当中基于模型的部分替换成不需要模型的部分就得到了今天的算法</li>
          <li>在这门课中，把 value iteration 和 policy iteration 统称为 model-base reinforcement learning，但是更准确来说，它们应该称为动态规划（dynamic programming）的方法。model-base reinforcement learning 简称&nbsp;MBRL，这个研究的是我用数据估计出一个模型，再基于这个模型进行强化学习。</li>
          <li>这节课介绍没有模型的强化学习方法，首先我们要学习随机变量的期望值，因为之前提到的 state value 和 action value 全都是随机变量的期望值，对随机变量采样的平均值可以作为 E[X] 的一个很好的近似。所以没有模型要有数据，没有数据要有模型才能学习。</li>
         </ul> 
         <p><strong>课程大纲：</strong></p> 
         <p>1.激励性实例（Motivating examples）：介绍蒙特卡洛估计（Mento Carlo Estimation）的基本思想</p> 
         <p>2.介绍三个基于蒙特卡洛（MC）强化学习的算法（这三个算法环环相扣，前一个是后一个的基础）</p> 
         <p>（1）最简单的基于 MC 的 RL 算法：MC basic（<span style="background-color:#fbd4d0;">我们把上节课介绍的 policy iteration 方法当中基于模型的部分替换成不需要模型的部分（依赖于数据的）就得到了这个算法。</span>是最简单的基于蒙特卡洛强化学习的算法，简单到这个算法在实际中不能用，因为效率很低，但他有利于揭示怎么样把模型给去掉，不基于模型来实现强化学习的这样一个核心idea，即它可以帮助理解之后的，因为强化学习是一环扣一环的）</p> 
         <p>（2）更高效地使用数据：MC Exploring Starts（把 MC basic 复杂化）</p> 
         <p>（3）MC 没有探索就启动：Algorithm: MC ε-Greedy（去除掉 exploring starts 这样的 assumption）</p> 
         <p class="img-center"><img alt="" height="292" src="https://i-blog.csdnimg.cn/blog_migrate/bc0f97f548491bd40f7eed6d42a0c165.png" width="332"></p> 
         <hr> 
         <h3 id="%E4%BA%8C.%E6%BF%80%E5%8A%B1%E6%80%A7%E5%AE%9E%E4%BE%8B%EF%BC%88Motivating%20examples%EF%BC%89">二.激励性实例（Motivating examples）</h3> 
         <ul>
          <li>从 model-based 强化学习过渡到 model-free 的强化学习，最难以理解的就是我们如何在没有模型的情况下去估计一些量？（How can we estimate something without models）</li>
          <li>最简单的方法：蒙特卡洛估算（Monte Carlo estimation）。</li>
         </ul> 
         <p>下面通过一个例子说明蒙特卡洛估算：投掷硬币</p> 
         <p>投掷硬币后的结果（正面或背面朝上）用随机变量（random variable） X 表示</p> 
         <ul>
          <li>如果结果为正面朝上，则 X = +1</li>
          <li>如果结果是背面朝上，则 X = -1</li>
         </ul> 
         <p><strong>目的是计算 E[X]（X 的平均数，X 的期望）。</strong></p> 
         <hr> 
         <p>这里有两种方法计算期望</p> 
         <p><strong>方法 1 ：基于模型的（model-based）</strong></p> 
         <p>假设概率模型为（我们知道随机变量（random variable） X 的概率分布（probability distribution））：正面朝上和背面朝上的概率都是 0.5</p> 
         <p class="img-center"><img alt="" height="52" src="https://i-blog.csdnimg.cn/blog_migrate/a6446f15a7f94f1b494f85a0c8fbd707.png" width="306"></p> 
         <p>那么随机变量（random variable） X 它的期望（expectation）就可以简单的通过定义计算：</p> 
         <p class="img-center"><img alt="" height="68" src="https://i-blog.csdnimg.cn/blog_migrate/a362bf615ac0bd77aa89c954f925b372.png" width="374"></p> 
         <p><strong>问题：</strong>可能无法知道精确的概率分布情况（precise distribution）！！</p> 
         <p><strong>方法 2&nbsp;：无模型的（model-free）</strong></p> 
         <p>基本思想：多次掷硬币，做很多次实验，得到很多的采样，然后计算所有采样的平均结果。</p> 
         <p>假设我们做了 N 次实验，这 N 次的实验结果分别是 x1,x2,...,xN，得到一个样本序列： {x1, x2, . , xN }。那么，均值可以近似为：</p> 
         <p class="img-center"><img alt="" height="157" src="https://i-blog.csdnimg.cn/blog_migrate/20caefd97b17894d41441869ed4656cb.png" width="472"></p> 
         <p>期望（expectation）用&nbsp;&nbsp;<img alt="\overline{x}" class="mathcode" src="https://latex.csdn.net/eq?%5Coverline%7Bx%7D">&nbsp;来近似，认为&nbsp;&nbsp;<img alt="\overline{x}" class="mathcode" src="https://latex.csdn.net/eq?%5Coverline%7Bx%7D">&nbsp;是 E(X)</p> 
         <p><span style="background-color:#fbd4d0;">这就是蒙特卡洛估计的基本思想！</span></p> 
         <hr> 
         <p><strong>问题：</strong>用蒙特卡洛估计（Mento Carlo Estimation）是否精确？</p> 
         <ul>
          <li>当 N 较小时，近似值不准确。</li>
          <li>随着 N 的增大，近似值会越来越精确。</li>
         </ul> 
         <p class="img-center"><img alt="" height="266" src="https://i-blog.csdnimg.cn/blog_migrate/470168117d85d1779db704c220cb548c.png" width="316"></p> 
         <p>如上图所示，我们已知真实的期望（expectation）是 0，随着做平均的样本数越多，样本的平均值（expectation）越接近真实的期望（expectation）0</p> 
         <hr> 
         <p>上面这样直观的解释有数学理论做支撑（大数定律）</p> 
         <p><span style="background-color:#fbd4d0;">iid：独立同分布样本（independent and identically distributed sample）</span></p> 
         <p><img alt="" height="886" src="https://i-blog.csdnimg.cn/blog_migrate/182b9a91cf27b661eb567dd5faaa8c49.png" width="1200"></p> 
         <hr> 
         <p><strong>总结：</strong></p> 
         <ul>
          <li><span style="background-color:#fbd4d0;">蒙特卡罗估计是指依靠重复随机抽样来解决近似问题的一大类技术。凡是需要做大量的采样实验，最后用实验的结果近似的的方法，都可以称为蒙特卡洛估计的方法。</span></li>
          <li><strong>我们为什么要已关注蒙特卡罗估计？因为它不需要模型</strong>！</li>
          <li>为什么要已关注均值估计（mean estimation）？为什么用蒙特卡洛来估计期望（expectation）？因为状态值（state value）和行动值（action value）被定义为随机变量的期望值（expectation）！</li>
         </ul> 
         <hr> 
         <h3 id="%E4%B8%89.%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%9F%BA%E4%BA%8E%20MC%20%E7%9A%84%20RL%20%E7%AE%97%E6%B3%95%EF%BC%9AMC%20basic">三.最简单的基于 MC 的 RL 算法：MC basic</h3> 
         <h4 id="1.%E5%B0%86%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%97%A0%E6%A8%A1%E5%9E%8B%E8%BF%AD%E4%BB%A3%EF%BC%88Convert%20policy%20iteration%20to%20be%20model-free%EF%BC%89">1.将策略迭代转换为无模型迭代（Convert policy iteration to be model-free）</h4> 
         <p>理解算法的关键是理解如何将策略迭代算法（policy iteration algorithm）转换为无模型算法（model-free）。我们知道策略迭代算法（policy iteration algorithm）是依赖于模型的，但是实际上我们可以把它依赖于模型的那部分给替换掉，替换成 model-free 的模块</p> 
         <ul>
          <li>应充分理解策略迭代（policy iteration algorithm）。</li>
          <li>应理解蒙特卡罗均值估计（Monte Carlo mean estimation）的思想。</li>
         </ul> 
         <hr> 
         <p><strong>接下来看策略迭代算法（policy iteration algorithm）如何转换为无模型（model-free）的：</strong></p> 
         <p>策略迭代（policy iteration algorithm）的每一次迭代都有两个步骤：</p> 
         <ul>
          <li>1.策略评估：我有一个策略&nbsp;πk，通过求解贝尔曼公式，我要求出来它的状态值（state value）v_πk</li>
          <li>2.策略改进：知道&nbsp;v_πk 之后就可以做改进，求解一个最优化问题得到一个新的策略&nbsp;πk+1。（通过选择最大的 q_πk 得到新的策略&nbsp;πk+1）</li>
         </ul> 
         <p style="text-align:center;"><img alt="" height="306" src="https://i-blog.csdnimg.cn/blog_migrate/46273636336eec7d5f48f030719e91c8.png" width="572"></p> 
         <p><strong>这里面非常核心的量是 q_πk (s, a)</strong></p> 
         <hr> 
         <p><strong>要计算动作值（action value） q_πk (s, a) 有两种算法：</strong></p> 
         <p><strong>方法 1 需要模型：</strong>这就是 value iteration 这个算法所使用的，第一步得到了&nbsp;v_πk，<span style="background-color:#fbd4d0;">第二步这些概率模型都是知道的</span>，所以就可以求出来 q_πk (s, a)<strong>（这些概率代表系统的模型）</strong></p> 
         <p class="img-center"><img alt="" height="108" src="https://i-blog.csdnimg.cn/blog_migrate/e0a9daee5dac5c65526a04ec724460b9.png" width="519"></p> 
         <p><strong>方法 2 不需要模型：</strong>这种方法依赖于动作值（action value） q_πk (s, a) 最最原始的定义。就是从当前状态 s 出发，选择动作 a 之后，我所得到的回报（return）Gt，这个 return 是一个随机变量，我求它的平均（average）或者求期望（expectation）就是动作值（action value）</p> 
         <p class="img-center"><img alt="" height="71" src="https://i-blog.csdnimg.cn/blog_migrate/889e798bdb43fe57afd75800a9996128.png" width="370"></p> 
         <p>理解：</p> 
         <ul>
          <li>老师在前面提到多，p这些概率是基于model的，model-free就是不用model，也就是不用到概率矩阵&nbsp;</li>
          <li>是的，也就是不需要知道概率分布，</li>
         </ul> 
         <blockquote> 
          <p>上面这个式子是基于蒙特卡洛方法的核心思想，这是一个均值估计（mean estimation）的问题，在第一部分（motivating example）中介绍了蒙特卡洛估计（Monte Carlo estimation）可以用来解决均值估计（mean estimation）的问题</p> 
         </blockquote> 
         <p><strong>实现无模型 RL 的思路：</strong>我们可以使用方法&nbsp;2 的表达式，根据数据（样本或经验）计算 q_πk(s,a)！</p> 
         <p class="img-center"><img alt="" height="67" src="https://i-blog.csdnimg.cn/blog_migrate/31b73ebfceb3f7a32c19aa7cc966a735.png" width="561"></p> 
         <hr> 
         <p><strong>下面来看一下具体是怎么求解的：</strong></p> 
         <p>动作值（action value）的蒙特卡洛估计（Monte Carlo estimation）程序：（episode就是根据一个给定策略进行的一次试验的轨迹(trajectory),在第一课里介绍过这个词）</p> 
         <p class="img-center"><img alt="" height="340" src="https://i-blog.csdnimg.cn/blog_migrate/39b566dc37c6373d0eef9e9185dbeeb7.png" width="501"></p> 
         <ul>
          <li>从任意一个 (s，a) 的组合开始，根据当前的策略 πk ，得到一个&nbsp;episode（ 在第一章中介绍了：当智能体按照策略（policy）与环境交互时，可能会在某些终端状态（terminal states）停止。由此产生的轨迹（trajectory）称为一集（an episode）（或一次试验 trail））</li>
          <li>计算出来这个 episode 对应的折扣回报（discounted return）g(s,a)</li>
          <li><span style="background-color:#fbd4d0;">Gt 是一个随机变量（random variable），g(s,a) 是这个随机变量 Gt 的一个采样</span></li>
         </ul> 
         <p class="img-center"><img alt="" height="51" src="https://i-blog.csdnimg.cn/blog_migrate/1d6156766109967825a0d77a52922614.png" width="338"></p> 
         <ul>
          <li>如果我们有很多这样的采样，有一个集合，我们就可以用这些采样 g 求一个平均值，来估计 Gt&nbsp; 的这个平均值，就是估计 Gt 的期望（expectation）。这个就是蒙特卡洛估计（Monte Carlo estimation），刚才在第一部分 motivating example 中介绍的</li>
         </ul> 
         <p class="img-center"><img alt="" height="106" src="https://i-blog.csdnimg.cn/blog_migrate/653370b703a6e7aec89e9311ab432633.png" width="512"></p> 
         <blockquote> 
          <p>基本理念： 当模型不可用时，我们可以使用数据。（没有模型就要有数据，没有数据就要有模型）（Fundamental idea: When model is unavailable, we can use data. ）这里的数据在统计或者是概率里面叫样本（sample），<strong>在强化学习中它有一个特殊的名字叫经验（experience）</strong></p> 
         </blockquote> 
         <hr> 
         <h4 id="2.The%20MC%20Basic%20algorithm">2.The MC Basic algorithm</h4> 
         <p>到此为止，算法已经逐渐清晰了，这个算法的名字叫：MC Basic algorithm，MC是蒙特卡洛首字母的缩写</p> 
         <p><strong>算法描述：</strong></p> 
         <p><img alt="" height="1046" src="https://i-blog.csdnimg.cn/blog_migrate/b0f9aa6b387b794827a008736035ca77.png" width="1200"></p> 
         <p>理解：我怎么记得policy evaluation是求状态值的？&nbsp;</p> 
         <ul>
          <li>因为求完状态值后，还是要求动作价值的，这里相当于合并了</li>
          <li>因为之前 policy evaluation 中求解状态值的目的就是在 policy improvement 这一步中使用状态值来求动作值，进一步求出下一步的新的策略&nbsp;πk+1（通过选择最大的 q_πk 得到新的策略&nbsp;πk+1）。这里 policy evaluation 中直接求出了动作值</li>
          <li>q值的均值就是状态值</li>
          <li>求解出station value后进行的操作还是计算action value，所以本质是一样的</li>
          <li>action value 就是要用到state value</li>
          <li>老师把从action value到state value放在下一步了</li>
         </ul> 
         <p><strong>伪代码：</strong></p> 
         <p><img alt="" height="1040" src="https://i-blog.csdnimg.cn/blog_migrate/5e904c2c6845889c7354aed7787fda9b.png" width="1200"></p> 
         <p>理解：每一个 episode 的 return 不是 model-based 的吗，否则要怎么算呢？</p> 
         <ul>
          <li>每个a得到很多轨迹和 return 然后估算 action value</li>
          <li>model-based直接算的就是期望，这个是采样</li>
          <li>这些轨迹是实验得来的不是算概率分布得来的 所以是model-free</li>
          <li>直接丢给环境，环境会返回结果，但是环境变化的分布我们是不知道的，根据返回结果experence来估计</li>
          <li>不知道模型是指你不知道模型的概率分布，但你做实验会知道结果</li>
          <li>就是和环境一直交互到一幕结束，把每一步的奖励累计就是return。不用模型</li>
          <li>这些return是与环境交互的结果吧，不需要知道model</li>
          <li>是用样本拟合的均值return</li>
          <li>你可以理解就是环境的动力学用一个外挂代理模块来实现，你算法只需要调用它就行</li>
         </ul> 
         <hr> 
         <ul>
          <li><span style="background-color:#fbd4d0;">MC Basic 是策略迭代算法（policy iteration algorithm）的一种变体，就是把基于 model 的模块拿掉，换成一个不需要 model 的模块。</span></li>
          <li>无模型算法是在基于模型算法的基础上建立起来的。因此，在研究无模型算法之前，有必要先了解基于模型的算法。</li>
          <li>要学习基于蒙特卡洛的强化学习的算法，首先应该明白基于模型的&nbsp;policy iteration 的算法</li>
          <li>MC Basic 有助于揭示基于&nbsp;MC 的无模型 RL（MC-based model-free RL）的核心思想，MC Basic 有助于揭示如何把 model-base 变成 model-free 的过程，但由于效率低（low efficiency）而不实用，之后还会介绍两个算法提高效率（efficiency）。之后会介绍怎么更高效的去更新数据，怎么去掉一些实际当中难以实现的假设等等，这会让算法看起来更复杂也更实用。</li>
          <li>既然策略迭代（policy iteration algorithm）是收敛的，那么 MC Basic 的收敛性也是有保证的，因为他俩类似，只是估计 action value 的方法有些差别。</li>
         </ul> 
         <p></p> 
         <p><strong>为什么 MC Basic 估算的是动作值而不是状态值？MC Basic 是直接来估计动作值（action value），而在 policy iteration 中是先估计了 state value，再转成 action value</strong></p> 
         <ul>
          <li>这是因为如果要估计 state value，之后还要转成 action value，那从 state value 到 action value 又依赖于模型，这是不行的，所以要直接把 action value 估计出来。状态值不能直接用于改进策略，当没有模型时，我们应该直接估计行动值。</li>
         </ul> 
         <hr> 
         <h4 id="3.%E4%BE%8B%E5%AD%90">3.例子</h4> 
         <h5 id="%EF%BC%881%EF%BC%89%E4%BE%8B%E5%AD%90%201">（1）例子 1</h5> 
         <p>在下面的例子中，有一个初始策略&nbsp;π0（图中绿色箭头），在其他状态策略不错，只是在状态 s1,s3 策略不太好。接下来就从&nbsp;π0 出发，用 MC Basic 算法找到一个最优的策略。</p> 
         <p class="img-center"><img alt="" height="348" src="https://i-blog.csdnimg.cn/blog_migrate/ff6be9ab3bf1a1ec7e46ae5be4eba32f.png" width="465"></p> 
         <p><strong>大纲：</strong>给定当前策略&nbsp;πk</p> 
         <p><strong>步骤1：</strong>策略评估（policy evaluation）：对任意的 s 和任意的 a，计算 q_πk (s, a)</p> 
         <ul>
          <li>在这个例子中有 9 个 state，每个 state 对应 5 个 action，有 45 个 state-action pairs，所以要找到 45 个 q_πk (s, a)，假设从每一个（s,a）出发都有 N 条轨迹，最后要求 N 条轨迹的平均的 return，那么一共有 45 × N 条轨迹。</li>
          <li>就是上节课那些例子里的Q值表，现在要把表填上。</li>
         </ul> 
         <p>理解：当目前有策略时，action不是也确定了？为什么有45个state-action？</p> 
         <ul>
          <li><span style="background-color:#fbd4d0;">不是，model才是概率，在free里没有给，但state，action，reward都还是已知的</span></li>
          <li>是45个(s, a)后再沿着策略PI_k走</li>
          <li>因为策略是条件概率</li>
          <li>因为你还得改进策略啊，要是你一直都算同一个动作就没法提升了。瞎挑的策略 还得更新呢</li>
          <li>都是确定策略的话那就确定了，随机性策略就得求平均了。。</li>
          <li>你不是要去explora一下其他策略的action value，来选最大的吗？跟你目前策略无关了</li>
          <li>有策略时V确定了，但是q(pi,k)还是没有确定啊，因为那些概率为0的策略也要计算的</li>
          <li>因为当下的策略并不一定是最优的，所以得从当前的策略出发进行优化，因此要全部遍历一遍</li>
          <li>那只是开始随便给的策略，你要继续优化啊</li>
          <li>action value和策略没有关系，受策略影响的是state value</li>
          <li>只有直接奖励和策略无关，而延迟奖励是要跟着策略走的，肯定和策略有关</li>
         </ul> 
         <p><strong>步骤2：</strong>策略改进（policy improvement）</p> 
         <ul>
          <li>在每个状态求出哪个 action 对应最大的 action value，就选择那个 action</li>
         </ul> 
         <p class="img-center"><img alt="" height="176" src="https://i-blog.csdnimg.cn/blog_migrate/7d633f053f823a4af1bf260e77a03415.png" width="523"></p> 
         <hr> 
         <p>由于篇幅有限，我们不可能把 45 个 q_πk 全部找到，我们只找 5 个，针对 s1 它的 5 个 action，只展示 qπk (s1, a)：</p> 
         <p><strong>步骤1：策略评估（policy evaluation）：</strong></p> 
         <ul>
          <li>刚才提到，如果我从一个（s1,a1）出发的话，要找 N 个轨迹，对这 N 条轨迹的 return 求平均，这样才能求出 q_πk(s1,a1)。但是由于当前的问题很简单，当前的策略（policy）是确定性的（deterministic），当前的环境也是确定性的（deterministic），<span style="background-color:#fbd4d0;">也就意味着，如果我从一个（s1,a1）出发，不管采样多少次，最后得到的轨迹都是相同的，</span>因此只采样一次就可以，因此只需 one episode 就能得到行动值！</li>
          <li>如果在更复杂的情况下，policy 是随机的（stochastic）；或者策略（policy）是确定性的（deterministic），但是环境是随机的（stochastic）。那么如果我从一个（s1,a1）出发，采样不同次就会得到不同的轨迹，那么就需要无限多的事件（episodes）（或至少很多事件（episodes））！需要采样多次，然后求一个平均</li>
         </ul> 
         <p>理解：环境是随机的是什么意思？</p> 
         <ul>
          <li>环境的随机性体现在两个方面: 一方面采取某个action后到达的下一个state不是确定的; 另一方面采取某个action后获得的reward不是确定的</li>
          <li>你可以理解为：从s1出发，执行了a1，结果不一定是s2,也可能是s5&nbsp;</li>
         </ul> 
         <p><img alt="" height="1049" src="https://i-blog.csdnimg.cn/blog_migrate/91f20a205234581dbe70a8bec9752d3b.png" width="1200"></p> 
         <p><img alt="" height="583" src="https://i-blog.csdnimg.cn/blog_migrate/a4ec04efe8252a2acbb22230584897da.png" width="1200"></p> 
         <p>理解：a5这里为啥后面都是-1了，不应该待在原地都是0吗</p> 
         <ul>
          <li>回答：在s1选择a5之后按照策略还是会“撞墙”，所以q(s1,a5)是如此&nbsp;</li>
          <li>S1产生a5原地不动所以第一个reward是 =0，但是之后都按照S1的policy（向上的action1），所以之后都是-1.</li>
          <li>呆在原地不懂一次后，按照当前的策略（在s1向上走），就会不断向上走又弹回s1，每一次奖励都是-1</li>
         </ul> 
         <p>理解：&nbsp;不是还有一个求平均值的过程吗，谁踢我一下</p> 
         <ul>
          <li>因为这里的当前策略对于每一个状态来说都是确定的，比如在状态s2，只可能往下走，而不可能往右走，所以无论采样多少次，得到的return都是一样的，所以只用采样一次，就不需要求平均了</li>
          <li>这个是determinstic的，取样就一次，取平均就是除以1，还是自己</li>
          <li>是有的，老师刚刚说了，由于我们环境是确定的，所以直接采样一次就行了</li>
          <li>这个是确定的，因为s1，a1对应的结果只有一种，所以采样只有一次，平均的分母N=1可以省略。如果s1，a1对应的结果是变化的，那就需要进行多次采样然后求平均，这时候N&gt;1就会显式表示平均的过程</li>
          <li>因为都是确定性策略，所以相当于每个episode都是一样，相应的q也一样，均值就是它本身</li>
          <li>这里只采样了一次，平均值就是这一条轨迹的值。</li>
          <li>因为策略本身是确定好的，只是第一次因为qvalue的原因强行确定了一次visit&nbsp;</li>
         </ul> 
         <p></p> 
         <p><strong>步骤2：策略改进（policy improvement）</strong></p> 
         <p>比较一下&nbsp;q_πk 对应的 action value 哪个最大，这里 a2,a3 最大</p> 
         <p class="img-center"><img alt="" height="108" src="https://i-blog.csdnimg.cn/blog_migrate/d4bc66d5f31463694304b3804ae3364c.png" width="405"></p> 
         <p>因为他俩对应的 action value 是一样的，所以可以任意选择 a2,a3 作为一个新的策略，直观上来看，往右走和往下走也是一个比较好的策略</p> 
         <p class="img-center"><img alt="" height="114" src="https://i-blog.csdnimg.cn/blog_migrate/75f85da749021b2a7f24c1bb9e78cf45.png" width="471"></p> 
         <p>s1 的这个新策略已经实现最优了，对于这个简单的例子，一个 iteration 就可以找到实现它的最优策略。这是因为除了 s1 之外，s1 周围的其他 state 已经达到最优了，所以 s1 也可以轻松找到最优策略。</p> 
         <hr> 
         <h5 id="%EF%BC%882%EF%BC%89%E4%BE%8B%E5%AD%902">（2）例子2</h5> 
         <p>这个例子不是展示如何用 MC basic 解决一个问题而是假如已经用 MC Basic 得到了一个最优策略，分析一下这个最优策略有什么性质</p> 
         <p>研究 episode 长度的影响（Examine the impact of episode length）：</p> 
         <ul>
          <li>在用 MC Basic 的时候需要数据，这个数据就是从任何一个状态和动作出发，有很多 episode，计算 episode 的 return。这个 episode 的长度理论上是越长越好，计算的 return 越精确，但现实中不能无限长，</li>
          <li>那么 episode 的长度应该设置为多长才合适？</li>
         </ul> 
         <p>例子设置：</p> 
         <p><img alt="" height="123" src="https://i-blog.csdnimg.cn/blog_migrate/153b160af5c45a870d2c69bdb34fd8ad.png" width="911"></p> 
         <p><strong>episode length 直观上可以理解为探索半径的长度&nbsp;</strong></p> 
         <p>问题：一个episode的终止条件是什么，假如在当前的policy下无法达到目标点的话，由于Discounted不是会一直持续下去吗</p> 
         <ul>
          <li>episode的长度是无穷的，需要我们手动终止</li>
          <li>实际仿真的时候需要设置终止条件或者step最大值</li>
         </ul> 
         <p><img alt="" height="1200" src="https://i-blog.csdnimg.cn/blog_migrate/59382d50be064cc03ac9c09452554e5b.png" width="1200"></p> 
         <p><img alt="" height="1200" src="https://i-blog.csdnimg.cn/blog_migrate/44f8e0f8a476735ed395862e331e6a3e.png" width="1200"></p> 
         <p><strong>研究结果</strong></p> 
         <ul>
          <li>当 episode 的长度较短时，只有靠近目标的状态具有非零状态值，只有离目标比较近的状态才能在这么短的步骤内找到目标，因此这些状态能找到最优策略。</li>
          <li>随着 episode 的长度逐渐增加，离目标越来越远的状态也能慢慢到达目标，从而找到最优策略。距离目标较近的状态比距离目标较远的状态更早出现非零状态值。</li>
          <li>episode 的长度应足够长，让所有状态都能有机会到达目标。</li>
          <li>episode 的长度不一定要无限长，充分长就够了。</li>
         </ul> 
         <hr> 
         <h3 id="%E5%9B%9B.%E6%9B%B4%E9%AB%98%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%EF%BC%88Use%20data%20more%20efficiently%EF%BC%89%EF%BC%9AMC%20Exploring%20Starts">四.更高效地使用数据（Use data more efficiently）：MC Exploring Starts</h3> 
         <p>这个算法是 MC Basic 的推广，可以让算法变得更加高效</p> 
         <p>MC Basic 算法：</p> 
         <ul>
          <li>优点：清晰揭示核心思想！帮助我们理解怎么样用蒙特卡洛方法实现不需要模型的强化学习</li>
          <li>缺点：过于简单，不实用，效率低。</li>
          <li>不过，MC Basic 可以扩展得更有效。</li>
         </ul> 
         <p>考虑一个网格世界的例子，按照策略 π，我们可以得到 an episode，例如</p> 
         <p class="img-center"><img alt="" height="84" src="https://i-blog.csdnimg.cn/blog_migrate/1d1243666944bd8b2e99e3a3d7ec829f.png" width="343"></p> 
         <p><strong>访问（Visit）：</strong>每当 episode 中出现一个状态-动作对（state-action pair）时，就称为对该状态-动作对的一次访问。</p> 
         <p>在 MC Basic 中使用数据的方法：初始访问法（Initial-visit method）</p> 
         <ul>
          <li>对于上图的这个 episode，我只考虑（s1,a2），用剩下的得到的 return 来估计（s1,a2）的 action value。只需计算回报值并近似计算 qπ（s1, a2）。</li>
          <li>MC Basic 算法就是这么做的。</li>
          <li>缺点：不能充分利用数据，有很多数据被浪费了。</li>
         </ul> 
         <p><strong>如何高效使用数据呢？</strong></p> 
         <ul>
          <li>我理解，MC-basic中每个episode只能拿到估计q值的一个数据，现在对每条episode拆分后，一个episode就能拿到估计不同state-action pair的多条数据。这样就算是充分利用数据了&nbsp;</li>
          <li>一个trajectory可出现多次相同的s a pair</li>
          <li>这只是一个理论上提高数据利用率的改进的方法，还是要根据系统特点来看效果。如果研究对象系统状态转移非常稀疏，可能效果不好（利用不到几个片段），对于状态很容易循环的效果就可能很好</li>
         </ul> 
         <p><img alt="" height="1019" src="https://i-blog.csdnimg.cn/blog_migrate/032f5f08b9af7a6718af72f868fd7d76.png" width="1200"></p> 
         <p><strong>数据效率高的方法有两种：</strong></p> 
         <p><strong>first-visit method：</strong>上图中，状态-动作对 (s1,a2) 访问了两次，而&nbsp;first-visit method 只使用第一次访问&nbsp;(s1,a2) 的后面来估计 (s1,a1)，第二次出现的时候就不用它后面的来进行估计了。</p> 
         <p><strong>every-visit method：</strong>上图中，状态-动作对 (s1,a2) 访问了两次（第一次和第三次），every-visit method 只要访问了，不管是第几次，都可以用它后面的 return 估计 (s1,a2) 的 action value</p> 
         <p>理解：还是没懂first-visit和every-visit区别？</p> 
         <ul>
          <li>就是你估计时，对于episode里出现多次的（s，a），是把第一个（s，a）当作样本，还是把后面的（s，a）对都算作（s，a）样本</li>
         </ul> 
         <hr> 
         <p><strong>除了让数据的使用更加高效之外，我们还可以更加高效的去更新策略。基于 MC 的 RL 的另一个方面是何时更新策略。也有两种方法：</strong></p> 
         <p><strong>第一种方法是，</strong>在策略评估（policy evaluation）步骤中，收集从一个状态-行动对（state-action pair）出发的所有 episode，然后使用平均回报（average return）来近似估计动作值（action value）。</p> 
         <ul>
          <li>这是 MC Basic 算法所采用的方法。</li>
          <li>这种方法的问题在于，智能体必须等到（wait until）所有 episode 都收集完毕。这个等待的过程浪费时间，效率低。</li>
         </ul> 
         <p><strong>第二种方法是，</strong>使用单个 episode 的回报（return）来立刻估计动作值（action value），然后不要等待，下一步就直接开始改进策略。这样的话，我得到一个 episode 就改进策略，得到一个 episode 就改进策略，效率会提升。</p> 
         <ul>
          <li>In this way, we can improve the policy episode-by-episode.</li>
         </ul> 
         <hr> 
         <p>第二种方法会产生问题吗？</p> 
         <ul>
          <li>有人可能会说，单个 episode 的回报（return）无法准确地估计出相应的动作值（action value）。</li>
          <li>事实上，在上一章介绍的截断策略迭代算法（truncated policy iteration algorithm）中，我们已经做到了这一点，在&nbsp;truncated policy iteration algorithm 中的第一步做的是 policy evaluation，在那一步中要求出当前策略的 state value，求&nbsp;state value 要求解贝尔曼公式，又需要无穷多步迭代，当时在那个算法中我们只做有限步迭代，虽然得不到非常精确的 state value，但这个算法仍然可行。与现在这个思想类似，用一个 episode 来估计动作值（action value），这显然是不精确的，但是没关系。</li>
         </ul> 
         <p>这一些方法有名字，叫做&nbsp;Generalized policy iteration（简称 GPI）</p> 
         <ul>
          <li>GPI 不是一种特定的算法，是一大类算法，是一种思想，架构。</li>
          <li>它指的是在策略评估（policy-evaluation）和策略改进（policy-improvement）过程之间切换的总体思路或框架。而且策略评估（policy-evaluation）不需要非常精确的把&nbsp;action value 或者&nbsp;state value 估计出来。</li>
          <li>许多基于模型和无模型的 RL 算法都属于 GPI 这一框架。上节课和这节课的算法都可以属于 GPI 框架。</li>
         </ul> 
         <hr> 
         <p>有了上面的这些思考，如果我们能更高效地利用数据和更新估计值，就能得到一种新算法，即 MC Exploring Starts，这是我们之前学的 MC Basic 的推广</p> 
         <p>If we use data and update estimate more efficiently, we get a new algorithm called MC Exploring Starts:</p> 
         <p><img alt="" height="1192" src="https://i-blog.csdnimg.cn/blog_migrate/3fc58986a592ad916f3dac3492812070.png" width="1200"></p> 
         <hr> 
         <p>什么是探索起始（exploring starts）？</p> 
         <ul>
          <li><strong>探索（exploring）</strong>指的是我从每一个(s,a)出发，都要有 episode，只有这样我才能用后面生成的这些 reward 来估计 return，进一步估计 action value。如果恰恰有一个 state-action 没有被访问到，那我就可能把这个 action 给漏掉了，但是那个可能就是最优的，所以我们需要确保每个都被访问。</li>
          <li><strong>起始（starts）</strong>意味着我们要访问每个（s,a），从它后面能够生成 reward 的这些数据有两种方法：第一种方法是从每一个&nbsp;(s,a) 开始都有一个 episode，就是 <strong>start</strong>；第二种方法是从其他的 (s,a) 开始，但是也能经过当前的这个 (s,a)，那后面的这些数据也可以估计当前 (s,a) 的 return，这个叫<strong> visit</strong>。目前来讲，visit 方法没法确保，它依赖于策略和环境，没法确保从其他的 (s,a) 开始一定能够经过剩下的所有 (s,a)，下面我们介绍的新方法就是使得 visit 可以做到，这样就可以避免必须从每个 (s,a) 都开始的条件。</li>
          <li>MC Basic 和 MC exploring starts 都需要这一假设。</li>
         </ul> 
         <p class="img-center"><img alt="" height="157" src="https://i-blog.csdnimg.cn/blog_migrate/fb6570eb2b4b7ce2a37b781dcffaea57.png" width="570"></p> 
         <hr> 
         <p>为什么我们需要考虑探索起始（exploring starts）？</p> 
         <ul>
          <li>从理论上讲，只有对<strong>每个状态（state）的每个动作值（action value）</strong>都进行了充分的探索，我们才能正确地选择最优动作（optimal actions）。相反，如果没有探索某个动作，这个动作可能恰好是最优动作，从而被错过。</li>
          <li>在实践中，探索起始（exploring starts）是很难实现的。对于许多应用，尤其是涉及与环境物理交互的应用，很难收集到从每一对状态-行动开始的 episode。</li>
         </ul> 
         <p>因此，理论与实践之间存在差距。<br><strong>我们能否取消探索起始（exploring starts）的要求呢？接下来，我们将通过软策略（soft policies）来证明这一点。</strong></p> 
         <hr> 
         <h3 id="%E4%BA%94.MC%20%E6%B2%A1%E6%9C%89%E6%8E%A2%E7%B4%A2%E5%B0%B1%E5%90%AF%E5%8A%A8%EF%BC%88MC%20without%20exploring%20starts%EF%BC%89%EF%BC%9AAlgorithm%3A%20MC%20%CE%B5-Greedy">五.MC 没有探索就启动（MC without <strong>exploring starts</strong>）：Algorithm: MC ε-Greedy</h3> 
         <h4 id="1.%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D">1.算法介绍</h4> 
         <p>由上面的思考，引出了第五部分，如何把&nbsp;exploring starts 这个条件给去掉，这里给出了一个算法叫&nbsp;MC ε-Greedy</p> 
         <p><strong>什么是 soft policy？</strong></p> 
         <ul>
          <li>如果采取任何动作的概率都是正数，对每一个 action 都有可能去做选择，那么该政策就是软性的。</li>
          <li>policy 分为两种，一种是确定性的（deterministic）policy，<strong>之前讲的 greedy policy 就是deterministic</strong>；另一种是随机性的（stochastic），这里的 soft policy 包括后面要介绍的&nbsp;ε-Greedy 都是 stochastic policy</li>
         </ul> 
         <p><strong>为什么要引入 soft policy？</strong></p> 
         <ul>
          <li>如果采用软策略，如果我从一个 state-action pair 比如说 (s,a) 出发，如果后面的 episode 特别长，因为它是探索性的，我能够确保任何一个 s 和 a 都能被这个 episode 访问到。一些足够长的 episodes 就可以访问每一个状态-行动对。</li>
          <li>这样，我们就不需要从每个状态-行动对开始的大量 episodes 了。因此，可以取消探索起始（exploring starts）的要求。不需要从每一个 (s,a) 都出发了，只需要从一个或者几个出发，就能够覆盖到其他的。</li>
         </ul> 
         <p><strong>我们使用的是什么 soft policy？</strong></p> 
         <ul>
          <li>ε-greedy policies（除了这个还有其他软策略，当前使用的是这个而已）</li>
         </ul> 
         <p><strong>什么是&nbsp;ε-greedy policy？</strong></p> 
         <p><img alt="" height="925" src="https://i-blog.csdnimg.cn/blog_migrate/d200c8df609125ba46768b7dae6cae7c.png" width="1200"></p> 
         <hr> 
         <p><strong>为什么使用&nbsp;ε-greedy policy？</strong></p> 
         <ul>
          <li>平衡开发（或者叫充分利用）（exploitation）与探索（exploration）</li>
          <li><strong>充分利用 exploitation：</strong>比如我在一个状态，有很多 action，我知道里面有的 action 的 action value 比较大，那么我知道这些信息，下一时刻我就应该去采取那个 action，那么未来我相信我就能得到更多的 reward</li>
          <li><strong>探索（exploration）：</strong>我现在虽然知道那个 action 能够带来更多的 reward，但是也许现在的信息不完备，我应该去探索一下其他的 action，执行完之后可能发现其他的 action 的 action value 可能也是非常好的</li>
         </ul> 
         <p><img alt="" height="385" src="https://i-blog.csdnimg.cn/blog_migrate/b0341e8cfea53862f971c68a23566e92.png" width="1200"></p> 
         <hr> 
         <p><strong>如何将 ε-greedy&nbsp;嵌入基于 MC 的 RL 算法（基于蒙特卡洛的强化学习算法）？如何将两者结合？</strong></p> 
         <p><strong>最初，</strong>MC Basic 和 MC Exploring Starts 中的策略改进（policy improvement）步骤是求解下面的式子：（在第一步求解出了 q_πk (s, a)，这一步要求解下面这个优化问题得到一个新的策略）</p> 
         <p class="img-center"><img alt="" height="61" src="https://i-blog.csdnimg.cn/blog_migrate/5af16df7d3088e60dc4939c470533417.png" width="315"></p> 
         <p>之前我们没有强调过，但其实我们在求解这个优化问题的时候，π 应该在所有可能的策略当中去做选择，其中，Π 表示所有可能策略组成的集合。这里求解出的最优策略是：</p> 
         <p class="img-center"><img alt="" height="107" src="https://i-blog.csdnimg.cn/blog_migrate/0fff3d1911507576c1b972ee2bcf37bd.png" width="440"></p> 
         <p>上面的是我们之前的方法，现在要把&nbsp;ε-greedy&nbsp;嵌入基于 MC 的 RL 算法：</p> 
         <p><strong>现在，</strong>策略改进（policy improvement）步骤改为求解</p> 
         <p class="img-center"><img alt="" height="59" src="https://i-blog.csdnimg.cn/blog_migrate/494a9607c81b3843fba1f18119319baa.png" width="310"></p> 
         <p>在求解上面这个问题的时候，我不是在所有的策略里面去找，只是在 Πε 里面去找。其中，Πε 表示具有固定 ε 值的所有 ε-greedy 策略的集合（这里&nbsp;ε 是事先给定的）。</p> 
         <p>这时候所得到的最优策略是：（把最大的概率仍然给 greedy action，但是会给其他所有 action 都给一个相同的比较小的概率）</p> 
         <p class="img-center"><img alt="" height="95" src="https://i-blog.csdnimg.cn/blog_migrate/23433a6be4d58f248c0c9693526675a8.png" width="369"></p> 
         <ul>
          <li>这样我们就得到了 MC&nbsp;ε-greedy 的算法，MC ε-Greedy 与 MC Exploring Starts 相同，只是前者使用 ε-greedy 策略，后者使用 greedy 策略。</li>
          <li>它不需要探索起点（exploring starts）这样的一个条件，但仍需要以不同的形式访问所有的状态-动作对。It does not require exploring starts, but still requires to visit all state-action pairs in a different form.</li>
         </ul> 
         <hr> 
         <p><strong>伪代码：</strong></p> 
         <p><img alt="" height="1200" src="https://i-blog.csdnimg.cn/blog_migrate/1d7d0d4da77988d5167d132e36d66ab7.png" width="1200"></p> 
         <hr> 
         <h4 id="2.%E4%BE%8B%E5%AD%90">2.例子</h4> 
         <p>（1）讨论&nbsp; ε-greedy 的探索性：</p> 
         <p><strong>一个 episode 能访问所有状态-行动对吗？Can a single episode visit all state-action pairs?</strong></p> 
         <p>当 ε = 1 时，策略是均匀分布（uniform distribution）的，在我们的例子里面每一个状态有 5 个 action，每一个 action 都给了 0.2 的概率（由上面那个图片的最后一个公式可以计算出来）。当 ε = 1 时，探索能力（exploration ability）最强。</p> 
         <p>看图 (a)，从一点出发，根据策略要生成一个 episode，如果只有<strong> 100 步（episode有100步）</strong>的话，图 (a) 是它探索的情况，探索到了不少的 state；如果有1000步，图 (b) 可知，所有的 state 和 action 都被探索到了；如果有10000步，探索的次数会更大；图 (d) 是探索100万步的时候，每一个 state-action pair 被访问的次数，横轴代表 state-action pair 的索引，共有25个状态，每个状态有 5 个 action，所以一共有 125 个 state-action pair，这时候虽然只有一个 episode，但是每个 state-action pair 都被访问了很多次。</p> 
         <p><img alt="" height="401" src="https://i-blog.csdnimg.cn/blog_migrate/51c18723cded9da86fa44c78f80a0979.png" width="1003"></p> 
         <p>这个例子说明了，当&nbsp;ε 比较大的时候，探索性比较强，自然就不再需要用 exploring starts 这样的一个条件了，不需要从每一个 (s,a) 出发都有一个 episode，只需要从某一些 (s,a) 出发，我就能覆盖到其它所有的 (s,a)。</p> 
         <hr> 
         <p>当 ε 较小时，策略的探索能力（exploration ability）也较小。</p> 
         <p><img alt="" height="382" src="https://i-blog.csdnimg.cn/blog_migrate/1141e58381d05a1ff89386d5b90c9667.png" width="988"></p> 
         <p>图 (a)，步数比较小的时候，访问的 state 比较少，因为它的偏好比较强；即使我把 episode 的长度变成了 100 万步，他会呈现出非常不均匀的访问次数，有的 state value 被访问的次数多，有的少。不管怎么样，相比 greedy 来说，还是有一定的探索能力。</p> 
         <hr> 
         <p>下面用 MC ε-Greedy 算法实验一个例子：</p> 
         <p>运行 MC ε-Greedy 算法如下。在每次迭代中（In every iteration），执行下面的：</p> 
         <ul>
          <li>在 episode 生成步骤中，使用先前的 ε-Greedy 策略生成一个 episode，这个 episode 非常长，有一百万步！</li>
          <li>在其余步骤中，使用这一个 episode 更新所有的 state-action pair 它们相对应的 action value 和更新策略。</li>
          <li>之所以这样做，是想通过这个例子展示一下这个算法避开了 exploring starts 这样的一个条件，因为只要这个 episode 足够长，即使它从一个 state-action pair 出发，但仍然能够访问其他所有的&nbsp;state-action pair。</li>
          <li>两次迭代可以得到最优的 ε-Greedy 策略</li>
         </ul> 
         <p><img alt="" height="485" src="https://i-blog.csdnimg.cn/blog_migrate/42982a21f433176e5a62d12051d22a8c.png" width="995"></p> 
         <ul>
          <li>图 (a) 是最初的策略，这个策略是不好的，在每一个状态都有相同的概率去选择所有的 action。然后我用这样一个策略生成一个 100 万步长的 episode，然后更新策略，得到了图 (b) 这样的策略；这个策略仍然不够好，一些状态上会保持不动；然后再用这个策略得到一个 100 万步长的 episode，再去更新策略，就得到了图 (c) 这样的策略。</li>
          <li>我们看图 (c) 这样的策略，如果只看图上绿色箭头概率最大的 action（箭头最长的方向），比图 (b) 策略来说相对而言比较合理，从任何一点出发都能到达目标，但是它们会穿过障碍物，所以从这个意义上讲它还不是最优的，因为最优的应该绕过障碍物到达目标。所以 ε-Greedy 通过探索性得到了一些好处，但是牺牲掉了最优性。</li>
         </ul> 
         <hr> 
         <p><strong>与贪婪策略（greedy policy）相比</strong></p> 
         <ul>
          <li>ε-greedy 策略的优势在于它们具有更强的探索能力，因此不需要探索开始（exploring starts）的条件。</li>
          <li>缺点是 ε-greedy 策略一般来说不是最优的（not optimal）（我们只能证明总是存在最优的 greedy 策略）。</li>
          <li>实际中可以设置应该比较小的&nbsp;ε 值，当这个&nbsp;ε 趋于 0 的时候，ε-greedy 就接近于 greedy，所以用这个算法找到的最优的&nbsp; ε-greedy 策略就接近于最优的&nbsp; greedy 策略；在实际中也可以让&nbsp;ε 逐渐减小，比如在开始的时候&nbsp;&nbsp;ε 比较大，就有比较强的探索能力，然后让&nbsp;&nbsp;ε 逐渐趋于 0，最后得到的策略又又比较好的最优性。</li>
          <li>MC ε-greedy 算法给出的最终策略只有在所有 ε-greedy 策略的集合 Πε 中才是最优的。</li>
          <li>ε不能太大。</li>
         </ul> 
         <hr> 
         <p>下面我们用几个例子说明一下 ε-greedy 的最优性</p> 
         <p><img alt="" height="105" src="https://i-blog.csdnimg.cn/blog_migrate/fad2c07805aa1c85ee242ec47cbb53f8.png" width="960"></p> 
         <p>在下面的例子中，首先给出左图绿色箭头的一个策略，然后求解它的贝尔曼公式，可以得到它的 state value（右图）。</p> 
         <p><img alt="" height="598" src="https://i-blog.csdnimg.cn/blog_migrate/c5c9e5ba99980af85ba4a57c7e21b01b.png" width="941"></p> 
         <ul>
          <li>左上角第一幅图的 ε = 0，它是一个 greedy 的策略，并且它是在这个情况下最优的一个策略。</li>
          <li>右上角的第二幅图 ε = 0.1，是第二个策略，第二个策略与第一个策略的关系是，他们是一致的（consistent），因为左下角那个小格子上，第一幅图中是往上走，第二幅图中最大的概率（最长的绿色箭头）是往上走，但与从同时也给其他的 action 有一些比较小的概率（小箭头），这个就被称为 consistent，这时候使用左图策略算出的 state value 比第一幅图的小，就是因为在很多地方它采取了不该采取的措施。</li>
          <li><strong>consistent：</strong>右上角的第二幅图 ε = 0.1，是第二个策略，第二个策略与第一个策略的关系是，他们是一致的（consistent），因为在第二幅图的左图中的任意一个状态中，箭头最长的策略（具有最大概率的策略）和第一幅图左图的策略一样。</li>
          <li>进一步增大&nbsp;ε，state value 变得更小，我们知道&nbsp;state value 可以用来衡量一个策略是否好，最优策略的定义就是基于&nbsp;state value，最优策略是有最大的&nbsp;state value</li>
          <li>因此虽然所有策略都和最优的 greedy 策略保持一致（绿色箭头一样），但最优性变得越来越差</li>
          <li>实际上 target area 上的 state value 在最优的策略里，应该是最大的值，但当&nbsp;ε 比较大的时候，它反而变成最小的值，变成负数，因为它在这个地方有比较大的概率进入 forbidden area，得到很多对负数 reward。&nbsp;</li>
         </ul> 
         <hr> 
         <p>下面例子说明 MC ε-Greedy 算法中&nbsp;ε 的选择不能太大。或者最开始的时候&nbsp;ε 比较大，探索性比较强，最后&nbsp;ε 逐渐减小到 0，就可以得到一个最优的策略</p> 
         <p><img alt="" height="743" src="https://i-blog.csdnimg.cn/blog_migrate/dba6a400df09cba8ee83de3903320763.png" width="991"></p> 
         <ul>
          <li>左上角第一幅图的 ε = 0，它是一个 greedy 的策略，并且它是在这个情况下最优的一个策略。</li>
          <li>用 MC ε-Greedy 得到的 optimal&nbsp;ε-Greedy 策略，得到这个策略之后要用到实际中，实际中不会带有&nbsp;ε（即右上角的左图不会有小箭头了，只保留概率最大的策略，即只保留大箭头），会把它转成对应的 greedy 策略，这个转换成的 greedy 策略，我希望它与最优策略（左上角的左图策略）相同，即我们希望是具有<strong>一致性的（consistent）</strong>。在&nbsp;ε=0.1 的时候，把它转成 greedy 的时候与左上角左图相同，就是 greedy 的策略。但当&nbsp;ε=0.2 或更大的时候，ε-Greedy 策略与左上角的 greedy 策略无关了，转换过去（即只取最长的箭头，不要小箭头）他俩就不同了</li>
          <li><span style="background-color:#fbd4d0;">因此如果你想用 MC&nbsp;ε-Greedy的话，那么你的 ε 选择不能太大。或者用一个技巧，在最开始的时候 ε 比较大，探索性比较强，最后&nbsp;ε 逐渐减小到 0，就可以得到一个最优的策略</span></li>
         </ul> 
         <h3 id="%C2%A0%E5%85%AD.%E6%80%BB%E7%BB%93">&nbsp;六.总结</h3> 
         <p><img alt="" height="455" src="https://i-blog.csdnimg.cn/blog_migrate/b6253b903de1b98532d6d44d88438db6.png" width="697"></p> 
         <p></p> 
         <p></p> 
         <p></p> 
         <p></p> 
         <p></p> 
         <p></p> 
         <p></p> 
        </div> 
       </div> 
      </article>   
      <img src="https://img-home.csdnimg.cn/images/20211209110851.png" alt="pdf_watermark" width="0" height="0" style="display: none"> 
      <div class="print_watermark"></div> 
      <div class="print_watermark_info"> 
       <p>内容来源：csdn.net</p> 
       <p>作者昵称：leaf_leaves_leaf</p> 
       <p>原文链接：https://blog.csdn.net/qq_64671439/article/details/135345465</p> 
       <p>作者主页：https://blog.csdn.net/qq_64671439</p> 
      </div> 
      <style>
    .print_watermark, .print_watermark_info {
      display: none
    }
    @media print {
      body {
        -webkit-print-color-adjust: exact; /* Chrome, Safari */
        color-adjust: exact; /* Firefox */
        background-image: none !important;
      }
      * {
        -webkit-print-color-adjust: exact;
      }
      .blog-content-box {
        padding: 0;
      }
      .blog-content-box .article-header .article-info-box > div:not(.article-bar-top){
        display: none !important;
      }
      .blog-content-box .article-header .article-info-box  .article-bar-top img{
          display:none
      }
      .blog-content-box .article-header .article-info-box > .article-bar-top .bar-content > *:not(.follow-nickName):not(.time){
          display: none !important;
      }
      .print_watermark {
        display: block;
        position: fixed;
        top: 0;
        left: 0;
        bottom: 0;
        right: 0;
        z-index: 999;
        background-image: url('https://img-home.csdnimg.cn/images/20211209110851.png');
        background-size: 180px auto;
        background-repeat: repeat;
      }
      .print_watermark_info {
        display: block;
        position: fixed;
        bottom: 16px;
        right: 0;
        z-index: 1000;
        color: #e8e8ed;
        font-size: 12px;
        ocapity: .5
      }
      @page {
        margin: 0 10mm 10mm;
        size: landscape;
      }
      body, article {
        width: 100%;
        margin: 0;
        padding: 0;
      }
      #csdn-toolbar,.main_father > *:not(#mainBox), .csdn-side-toolbar, .main_father aside {
        display: none !important;
      }
      .main_father > #mainBox {
        width: unset
      }
      .main_father > #mainBox > main > *:not(.blog-content-box){
        display: none !important;
      }
    }
  </style> 
     </div> 
     <div class="directory-boxshadow-dialog" style="display:none;"> 
      <div class="directory-boxshadow-dialog-box"> 
      </div> 
      <div class="vip-limited-time-offer-box-new" id="vip-limited-time-offer-box-new"> 
       <img class="limited-img limited-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-newWhite.png"> 
       <div class="vip-limited-time-top">
         确定要放弃本次机会？ 
       </div> 
       <span class="vip-limited-time-text">福利倒计时</span> 
       <div class="limited-time-box-new"> 
        <span class="time-hour"></span> 
        <i>:</i> 
        <span class="time-minite"></span> 
        <i>:</i> 
        <span class="time-second"></span> 
       </div> 
       <div class="limited-time-vip-box"> 
        <p> <img class="coupon-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-roup.png"> <span class="def">立减 ¥</span> <span class="active limited-num"></span> </p> 
        <span class="">普通VIP年卡可用</span> 
       </div> 
       <a class="limited-time-btn-new" href="https://mall.csdn.net/vip" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.9621&quot;}" data-report-query="spm=1001.2101.3001.9621">立即使用</a> 
      </div> 
     </div> 
     <div class="more-toolbox-new more-toolbar" id="toolBarBox"> 
      <div class="left-toolbox"> 
       <div class="toolbox-left"> 
        <div class="profile-box"> 
         <a class="profile-href" target="_blank" href="https://blog.csdn.net/qq_64671439"><img class="profile-img" src="https://profile-avatar.csdnimg.cn/6ba86a546a1040ee8d58623f42066c13_qq_64671439.jpg!1"> <span class="profile-name"> leaf_leaves_leaf </span> </a> 
        </div> 
        <div class="profile-attend"> 
         <a class="tool-attend tool-bt-button tool-bt-attend" href="javascript:;" data-report-view="{&quot;mod&quot;:&quot;1592215036_002&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4232&quot;,&quot;extend1&quot;:&quot;已关注&quot;}">已关注</a> 
         <a class="tool-item-follow active-animation" style="display:none;">已关注</a> 
        </div> 
       </div> 
       <div class="toolbox-middle"> 
        <ul class="toolbox-list"> 
         <li class="tool-item tool-item-size tool-active is-like" id="is-like" data-type="bottom"> <a class="tool-item-href"> <img style="display:none;" id="is-like-imgactive-animation-like" class="animation-dom active-animation" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarThumbUpactive.png" alt=""> <img class="isactive" style="display:none" id="is-like-imgactive" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like-active.png" alt=""> <img class="isdefault" style="display:block" id="is-like-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like.png" alt=""> <span id="spanCount" class="count "> 52 </span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">点赞</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active is-unlike" id="is-unlike"> <a class="tool-item-href"> <img class="isactive" style="margin-right:0px;display:none" id="is-unlike-imgactive" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike-active.png" alt=""> <img class="isdefault" style="margin-right:0px;display:block" id="is-unlike-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike.png" alt=""> <span id="unlikeCount" class="count "></span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">踩</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active is-collection "> <a class="tool-item-href" href="javascript:;" data-report-click="{&quot;mod&quot;:&quot;popu_824&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4130&quot;,&quot;ab&quot;:&quot;new&quot;}"> <img style="display:none" id="is-collection-img-collection" class="animation-dom active-animation" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect-active.png" alt=""> <img class="isdefault" id="is-collection-img" style="display:block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect.png" alt=""> <img class="isactive" id="is-collection-imgactive" style="display:none" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCollectActive.png" alt=""> <span class="count get-collection " data-num="55" id="get-collection"> 55 </span> </a> 
          <div class="tool-hover-tip collect"> 
           <div class="collect-operate-box"> 
            <span class="collect-text" id="is-collection"> 收藏 </span> 
           </div> 
          </div> 
          <div class="tool-active-list"> 
           <div class="text">
             觉得还不错? 
            <span class="collect-text" id="tool-active-list-collection"> 一键收藏 </span> 
            <img id="tool-active-list-close" src="https://csdnimg.cn/release/blogv2/dist/pc/img/collectionCloseWhite.png" alt=""> 
           </div> 
          </div> </li> 
         <li class="tool-item tool-item-size tool-active tool-item-comment"> 
          <div class="guide-rr-first"> 
           <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward01.png" alt=""> 
           <button class="btn-guide-known">知道了</button> 
          </div> <a class="tool-item-href go-side-comment" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7009&quot;}"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/comment.png" alt=""> <span class="count"> 2 </span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">评论</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active tool-QRcode" data-type="article" id="tool-share"> <a class="tool-item-href" href="javascript:;" data-report-view="{&quot;spm&quot;:&quot;3001.4129&quot;,&quot;extra&quot;:{&quot;type&quot;:&quot;blogdetail&quot;}}"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/share.png" alt=""> <span class="count">分享</span> </a> 
          <div class="QRcode" id="tool-QRcode"> 
           <div class="share-bg-box"> 
            <div class="share-content"> 
             <a id="copyPosterUrl" data-type="link" class="btn-share">复制链接</a> 
            </div> 
            <div class="share-content"> 
             <a class="btn-share" data-type="qq">分享到 QQ</a> 
            </div> 
            <div class="share-content"> 
             <a class="btn-share" data-type="weibo">分享到新浪微博</a> 
            </div> 
            <div class="share-code"> 
             <div class="share-code-box" id="shareCode"></div> 
             <div class="share-code-text"> 
              <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/share/icon-wechat.png" alt="">扫一扫 
             </div> 
            </div> 
           </div> 
          </div> </li> 
         <li class="tool-item tool-item-size tool-active tool-item-reward"> <a class="tool-item-href" href="javascript:;" data-report-click="{&quot;mod&quot;:&quot;popu_830&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4237&quot;,&quot;dest&quot;:&quot;&quot;,&quot;ab&quot;:&quot;new&quot;}"> <img class="isdefault reward-bt" id="rewardBtNew" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/reward.png" alt="打赏"> <span class="count">打赏</span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">打赏</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active tool-downloadpdf" id="tool-downloadpdf"> <a class="tool-item-href" data-report-click="{&quot;spm&quot;:&quot;3001.6881&quot;,&quot;extra&quot;:&quot;{\&quot;type\&quot;:\&quot;hide\&quot;}&quot;}" data-report-view="{&quot;spm&quot;:&quot;3001.6881&quot;,&quot;extra&quot;:&quot;{\&quot;type\&quot;:\&quot;hide\&quot;}&quot;}"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/pdf.png" alt=""> </a> 
          <div class="tool-hover-tip">
           <span class="text">导出PDF</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active is-more" id="is-more"> <a class="tool-item-href"> <img class="isdefault" style="margin-right:0px;display:block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/more.png" alt=""> <span class="count"></span> </a> 
          <div class="more-opt-box"> 
           <div class="mini-box"> 
            <a class="tool-item-href" id="rewardBtNewHide" data-report-click="{&quot;spm&quot;:&quot;3001.4237&quot;,&quot;extra&quot;:&quot;{\&quot;type\&quot;:\&quot;hide\&quot;}&quot;}"> <img class="isdefault reward-bt" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/reward.png" alt="打赏"> <span class="count">打赏</span> </a> 
            <a class="tool-item-href" data-report-click="{&quot;spm&quot;:&quot;3001.6881&quot;,&quot;extra&quot;:&quot;{\&quot;type\&quot;:\&quot;hide\&quot;}&quot;}" data-report-view="{&quot;spm&quot;:&quot;3001.6881&quot;,&quot;extra&quot;:&quot;{\&quot;type\&quot;:\&quot;hide\&quot;}&quot;}"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/pdf.png" alt=""> <span class="count">导出PDF</span> </a> 
            <a class="tool-item-href" id="toolReportBtnHide"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png" alt=""> <span class="count">举报</span> </a> 
           </div> 
           <div class="normal-box"> 
            <a class="tool-item-href" id="toolReportBtnHideNormal"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png" alt=""> <span class="count">举报</span> </a> 
           </div> 
          </div> </li> 
        </ul> 
       </div> 
       <div class="toolbox-right"> 
        <div class="tool-directory"> 
         <a class="bt-columnlist-show" data-id="12540921" data-free="true" data-description="" data-subscribe="false" data-title="【强化学习的数学原理-赵世钰】课程笔记" data-img="https://i-blog.csdnimg.cn/columns/default/20201014180756916.png?x-oss-process=image/resize,m_fixed,h_224,w_224" data-url="https://blog.csdn.net/qq_64671439/category_12540921.html" data-sum="10" data-people="244" data-price="0" data-hotrank="0" data-status="true" data-oldprice="0" data-join="false" data-studyvip="true" data-studysubscribe="false" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.6334&quot;,&quot;extend1&quot;:&quot;专栏目录&quot;}" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.6334&quot;,&quot;extend1&quot;:&quot;专栏目录&quot;}">专栏目录</a> 
        </div> 
       </div> 
      </div> 
     </div>    
     <a id="commentBox" name="commentBox"></a> 
    </main> 
   </div> 
   <div class="recommend-right1  align-items-stretch clearfix" id="rightAsideConcision" data-type="recommend"> 
    <aside class="recommend-right_aside"> 
     <div id="recommend-right-concision"> 
      <div class="flex-column aside-box groupfile" id="groupfileConcision"> 
       <div class="groupfile-div1"> 
        <h3 class="aside-title">目录</h3> 
        <div class="align-items-stretch group_item"> 
         <div class="pos-box"> 
          <div class="scroll-box"> 
           <div class="toc-box"></div> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
     </div> 
    </aside> 
   </div> 
  </div> 
  <div class="mask-dark"></div> 
  <div class="skin-boxshadow"></div> 
  <div class="directory-boxshadow"></div> 
  <div style="display:none;"> 
   <img src="" onerror="setTimeout(function(){if(!/(csdn.net|iteye.com|baiducontent.com|googleusercontent.com|360webcache.com|sogoucdn.com|bingj.com|baidu.com)$/.test(window.location.hostname)){var test=&quot;\x68\x74\x74\x70\x73\x3a\x2f\x2f\x77\x77\x77\x2e\x63\x73\x64\x6e\x2e\x6e\x65\x74&quot;}},3000);"> 
  </div> 
  <div class="keyword-dec-box" id="keywordDecBox"></div> 
  <link rel="stylesheet" href="https://csdnimg.cn/release/blog_editor_html/release1.6.12/ckeditor/plugins/chart/chart.css">        
  <link rel="stylesheet" href="https://g.csdnimg.cn/lib/cboxEditor/1.1.6/embed-editor.min.css"> 
  <link rel="stylesheet" href="https://csdnimg.cn/release/blog_editor_html/release1.6.12/ckeditor/plugins/codesnippet/lib/highlight/styles/atom-one-dark.css">                  
 </body>
</html>